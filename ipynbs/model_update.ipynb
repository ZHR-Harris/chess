{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "#currentpath = os.path.dirname(os.path.realpath(__file__))\n",
    "project_basedir = '..'#os.path.join(currentpath,'..')\n",
    "sys.path.append(project_basedir)\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from common.utils import Dataset,ProgressBar\n",
    "from tflearn.data_flow import DataFlow,DataFlowStatus,FeedDictFlow\n",
    "from tflearn.data_utils import Preloader,ImagePreloader\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import common\n",
    "import tflearn\n",
    "import copy\n",
    "from config import conf\n",
    "from cchess import *\n",
    "from gameplays.game_convert import convert_game,convert_game_value,convert_game_board\n",
    "import os, shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "from net.net_maintainer import NetMatainer\n",
    "from net import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 16 21:23:49 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 45%   77C    P2    97W / 250W |   2124MiB / 11172MiB |     64%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:42:00.0 Off |                  N/A |\r\n",
      "| 24%   44C    P8    17W / 250W |    805MiB / 11172MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     28156      C   /usr/local/bin/python3                       153MiB |\r\n",
      "|    0     28159      C   /usr/local/bin/python3                       153MiB |\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a network predict select and move of Chinese chess, with minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stamp = time.strftime('%Y-%m-%d_%H-%M-%S',time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_dir = os.path.join(conf.history_selfplay_dir,stamp)\n",
    "data_dir = '../data/history_selfplays/2018-06-22_00-44-48/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir already exist: ../data/history_selfplays/2018-06-22_00-44-48/\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(data_dir):\n",
    "    print('data_dir already exist: {}'.format(data_dir))\n",
    "else:\n",
    "    print('creating data_dir: {}'.format(data_dir))\n",
    "    os.mkdir(\"{}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_CORE = [1]\n",
    "BATCH_SIZE = 512\n",
    "BEGINING_LR = 0.01\n",
    "#TESTIMG_WIDTH = 500\n",
    "model_name = 'update_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribute_dir = conf.distributed_datadir\n",
    "filelist = os.listdir(data_dir)\n",
    "#filelist = os.listdir(data_dir)\n",
    "#filelist = [os.path.join(distribute_dir,i) for i in filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_dir = conf.distributed_server_weight_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in filelist:\n",
    "    src = os.path.join(distribute_dir,f)\n",
    "    dst = os.path.join(data_dir,f)\n",
    "    shutil.move(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filelist = [os.path.join(data_dir,i) for i in filelist]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filelist[0].split('.')[-2].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61653"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filelist = filelist[:1000]\n",
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = common.board.create_uci_labels()\n",
    "label2ind = dict(zip(labels,list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pgn2value = dict(pd.read_csv('./data/resultlist.csv').values[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_ab = dict(zip('abcdefghi','abcdefghi'[::-1]))\n",
    "rev_num = dict(zip('0123456789','0123456789'[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElePreloader(object):\n",
    "    def __init__(self,filelist,batch_size=64):\n",
    "        self.batch_size=batch_size\n",
    "        #content = pd.read_csv(datafile,header=None,index_col=None)\n",
    "        self.filelist = filelist#[i[0] for i in content.get_values()]\n",
    "        self.pos = 0\n",
    "        self.feature_list = {\"red\":['A', 'B', 'C', 'K', 'N', 'P', 'R']\n",
    "                             ,\"black\":['a', 'b', 'c', 'k', 'n', 'p', 'r']}\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = self.iter()\n",
    "        assert(len(self.filelist) > batch_size)\n",
    "        #self.game_iterlist = [None for i in self.filelist]\n",
    "    \n",
    "    def iter(self):\n",
    "        retx1,rety1,retx2,rety2 = [],[],[],[]\n",
    "        vals = []\n",
    "        filelist = []\n",
    "        num_filepop = 0\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                filelist = copy.copy(self.filelist)\n",
    "                random.shuffle(filelist)\n",
    "                #if self.game_iterlist[i] == None:\n",
    "                #    if len(filelist) == 0:\n",
    "                #        filelist = copy.copy(self.filelist)\n",
    "                #        random.shuffle(filelist)\n",
    "                #    self.game_iterlist[i] = convert_game_value(filelist.pop(),self.feature_list,None)\n",
    "                #    num_filepop += 1\n",
    "                #game_iter = self.game_iterlist[i]\n",
    "                \n",
    "                #x1,y1,val1 = game_iter.__next__()\n",
    "                for one_file in filelist:\n",
    "                    try:\n",
    "                        for x1,y1,val1 in convert_game_value(one_file,self.feature_list,None):\n",
    "                            x1 = np.transpose(x1,[1,2,0])\n",
    "                            x1 = np.expand_dims(x1,axis=0)\n",
    "\n",
    "                            #if random.random() < 0.5:\n",
    "                            #    y1 = [rev_ab[y1[0]],y1[1],rev_ab[y1[2]],y1[3]]\n",
    "                            #    x1 = x1[:,:,::-1,:]\n",
    "                            #    #x1 = np.concatenate((x1[:,::-1,:,7:],x1[:,::-1,:,:7]),axis=-1)\n",
    "                            retx1.append(x1)\n",
    "                            #rety1.append(y1)\n",
    "                            oney = np.zeros(len(labels))\n",
    "                            oney[label2ind[''.join(y1)]] = 1\n",
    "                            rety1.append(oney)\n",
    "                            vals.append(val1)\n",
    "\n",
    "                            if len(retx1) >= self.batch_size:\n",
    "                                yield (np.concatenate(retx1,axis=0),np.asarray(rety1),np.asarray(vals),num_filepop)\n",
    "                                retx1,rety1 = [],[]\n",
    "                                vals = []\n",
    "                                num_filepop = 0\n",
    "                    except:\n",
    "                        print(one_file)\n",
    "                        import traceback  \n",
    "                        traceback.print_exc()  \n",
    "                        continue\n",
    "                    num_filepop += 1\n",
    "                    #print(one_file)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        #pass\n",
    "        x1,y1,val1,num_filepop = self.batch_iter.__next__()\n",
    "        return x1,y1,val1,num_filepop\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/history_selfplays/2018-06-22_00-44-48/2018-06-25_06-40-43_428_mcts-mcts_net-net_w.cbf'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = ElePreloader(filelist=filelist,batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE[0])):\n",
    "    coord = tf.train.Coordinator()\n",
    "    trainflow = FeedDictFlow({\n",
    "            'data':trainset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=False,continuous=True,num_threads=1)\n",
    "trainflow.start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "testset = ElePreloader(datafile='data/test_list.csv',batch_size=BATCH_SIZE)\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE[0])):\n",
    "    coord = tf.train.Coordinator()\n",
    "    testflow = FeedDictFlow({\n",
    "            'data':testset,\n",
    "        },coord,batch_size=BATCH_SIZE,shuffle=True,continuous=True,num_threads=1)\n",
    "testflow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [ 1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1\n",
      "  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1\n",
      "  1 -1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n",
      " -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "sample_x1,sample_y1,sample_value,sample_num = trainflow.next()['data']\n",
    "print(sample_num,sample_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/history_selfplays/2018-06-22_00-44-48/2018-06-22_10-33-21_725_mcts-mcts_net-net_w.cbf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.filelist[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7\n",
      "../data/history_selfplays/2018-06-22_00-44-48/2018-06-24_01-17-02_124_mcts-mcts_net-net_w.cbf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-7ebe7c9d2c16>\", line 50, in iter\n",
      "    yield (np.concatenate(retx1,axis=0),np.asarray(rety1),np.asarray(vals),num_filepop)\n",
      "GeneratorExit\n",
      "Exception ignored in: <generator object ElePreloader.iter at 0x7f4b4193b7d8>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    }
   ],
   "source": [
    "filepops = []\n",
    "for sample_x1,sample_y1,sample_value,num_filepop in trainset.iter():\n",
    "    #xx = x1\n",
    "    filepops.append(num_filepop)\n",
    "    print(len(filepops),num_filepop)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample_x1,sample_y1,sample_value = testflow.next()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 10, 9, 14), (512, 2086), (512,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x1.shape,sample_y1.shape,sample_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2h5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(sample_y1[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sample_x1[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"models\": 文件已存在\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"models/{}\".format(model_name)):\n",
    "    os.mkdir(\"models/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BATCH = len(trainset)\n",
    "#N_BATCH_TEST = 300 * (128 / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61653"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61653"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_BATCH#,N_BATCH_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-06-20_09-00-14'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_netname = NetMatainer(None,network_dir).get_latest()\n",
    "latest_netname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from net.resnet import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "[None, 10, 9, 128]\n",
      "inside res net [None, 10, 9, 128]\n",
      "INFO:tensorflow:Restoring parameters from /work/icybee/icyChessZero/config/../data/prepare_weight/2018-06-20_09-00-14\n"
     ]
    }
   ],
   "source": [
    " (sess,graph),((X,training),(net_softmax,value_head,train_op_policy,train_op_value,policy_loss,accuracy_select,global_step,value_loss,nextmove,learning_rate,score)) = \\\n",
    "    get_model('{}/{}'.format(conf.distributed_server_weight_dir,latest_netname),labels,GPU_CORE=GPU_CORE,FILTERS=128,NUM_RES_LAYERS=7,extra=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with graph.as_default():\n",
    "#    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with graph.as_default():\n",
    "    train_epoch = 58\n",
    "    train_batch = 0\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    saver.restore(sess,\"models/{}/model_{}\".format(model_name,train_epoch - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_epoch = 1\n",
    "train_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 STEP 10708 LR 0.01 ACC 29.18 LOSS 2.7 value_loss 0.48 71.16 % [===================================>---------------] 43873/61653 \t used:6354s eta:2575 s../data/history_selfplays/2018-06-22_00-44-48/_blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-18-e54fdbcb14dd>\", line 34, in iter\n",
      "    for x1,y1,val1 in convert_game_value(one_file,self.feature_list,None):\n",
      "  File \"../gameplays/game_convert.py\", line 68, in convert_game_value\n",
      "    doc = xmltodict.parse(open(onefile,encoding='utf-8').read())\n",
      "  File \"/usr/local/lib/python3.6/site-packages/xmltodict.py\", line 330, in parse\n",
      "    parser.Parse(xml_input, True)\n",
      "xml.parsers.expat.ExpatError: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 STEP 15023 LR 0.01 ACC 30.43 LOSS 2.58 value_loss 0.46 99.84 % [=================================================>-] 61554/61653 \t used:8915s eta:14 sss../data/history_selfplays/2018-06-22_00-44-48/2018-06-22_09-07-48_171_mcts-mcts_net-net_peace.cbf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-18-e54fdbcb14dd>\", line 50, in iter\n",
      "    yield (np.concatenate(retx1,axis=0),np.asarray(rety1),np.asarray(vals),num_filepop)\n",
      "GeneratorExit\n",
      "Exception ignored in: <generator object ElePreloader.iter at 0x7fa57c0c7888>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 STEP 14936 LR 0.01 ACC 32.84 LOSS 2.37 value_loss 0.43 99.23 % [=================================================>-] 61176/61653 \t used:8924s eta:69 sss../data/history_selfplays/2018-06-22_00-44-48/_blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-18-e54fdbcb14dd>\", line 34, in iter\n",
      "    for x1,y1,val1 in convert_game_value(one_file,self.feature_list,None):\n",
      "  File \"../gameplays/game_convert.py\", line 68, in convert_game_value\n",
      "    doc = xmltodict.parse(open(onefile,encoding='utf-8').read())\n",
      "  File \"/usr/local/lib/python3.6/site-packages/xmltodict.py\", line 330, in parse\n",
      "    parser.Parse(xml_input, True)\n",
      "xml.parsers.expat.ExpatError: no element found: line 1, column 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 STEP 15023 LR 0.01 ACC 32.55 LOSS 2.41 value_loss 0.46 99.84 % [=================================================>-] 61555/61653 \t used:8976s eta:14 s../data/history_selfplays/2018-06-22_00-44-48/2018-06-20_03-28-26_444_mcts-mcts_net-net_b.cbf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-18-e54fdbcb14dd>\", line 50, in iter\n",
      "    yield (np.concatenate(retx1,axis=0),np.asarray(rety1),np.asarray(vals),num_filepop)\n",
      "GeneratorExit\n",
      "Exception ignored in: <generator object ElePreloader.iter at 0x7fa57c0c7888>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    }
   ],
   "source": [
    "restore = True\n",
    "N_EPOCH = 3\n",
    "DECAY_EPOCH = 20\n",
    "\n",
    "class ExpVal:\n",
    "    def __init__(self,exp_a=0.97):\n",
    "        self.val = None\n",
    "        self.exp_a = exp_a\n",
    "    def update(self,newval):\n",
    "        if self.val == None:\n",
    "            self.val = newval\n",
    "        else:\n",
    "            self.val = self.exp_a * self.val + (1 - self.exp_a) * newval\n",
    "    def getval(self):\n",
    "        return round(self.val,2)\n",
    "    \n",
    "expacc_move = ExpVal()\n",
    "exploss = ExpVal()\n",
    "expsteploss = ExpVal()\n",
    "\n",
    "begining_learning_rate = 1e-2\n",
    "\n",
    "pred_image = None\n",
    "if restore == False:\n",
    "    train_epoch = 1\n",
    "    train_batch = 0\n",
    "for one_epoch in range(train_epoch,N_EPOCH):\n",
    "    trainset = ElePreloader(filelist=filelist,batch_size=BATCH_SIZE)\n",
    "    train_epoch = one_epoch\n",
    "    pb = ProgressBar(worksum=N_BATCH,info=\" epoch {} batch {}\".format(train_epoch,train_batch))\n",
    "    pb.startjob()\n",
    "    \n",
    "    #for one_batch in range(N_BATCH):\n",
    "    one_batch = 0\n",
    "    for batch_x,batch_y,batch_v,one_finish_sum in trainset.iter():\n",
    "        one_batch += 1\n",
    "        if pb.finishsum > pb.worksum - 100: # 100 buffer\n",
    "            break\n",
    "        \n",
    "        #batch_x,batch_y,batch_v = trainflow.next()['data']\n",
    "        batch_v = np.expand_dims(np.nan_to_num(batch_v),1)\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 2 ** -(one_epoch // DECAY_EPOCH)\n",
    "        with graph.as_default():\n",
    "            _,step_loss,step_acc_move,step_value = sess.run(\n",
    "                [train_op_policy,policy_loss,accuracy_select,global_step],feed_dict={\n",
    "                    X:batch_x,nextmove:batch_y,learning_rate:batch_lr,training:True,\n",
    "                })\n",
    "            _,step_value_loss,step_val_predict = sess.run(\n",
    "                [train_op_value,value_loss,value_head],feed_dict={\n",
    "                    X:batch_x,learning_rate:batch_lr,training:True,score:batch_v,\n",
    "                })\n",
    "            #batch_v = - batch_v\n",
    "            #batch_x = np.concatenate((batch_x[:,::-1,:,7:],batch_x[:,::-1,:,:7]),axis=-1)\n",
    "            #_,step_value_loss,step_val_predict = sess.run(\n",
    "            #    [train_op_value,value_loss,value_head],feed_dict={\n",
    "            #        X:batch_x,learning_rate:batch_lr,training:True,score:batch_v,\n",
    "            #    })\n",
    "            \n",
    "        \n",
    "        step_acc_move *= 100\n",
    "        \n",
    "        expacc_move.update(step_acc_move)\n",
    "        exploss.update(step_loss)\n",
    "        expsteploss.update(step_value_loss)\n",
    "\n",
    "       \n",
    "        pb.info = \"EPOCH {} STEP {} LR {} ACC {} LOSS {} value_loss {}\".format(\n",
    "            one_epoch,one_batch,batch_lr,expacc_move.getval(),exploss.getval(),expsteploss.getval())\n",
    "        \n",
    "        pb.complete(one_finish_sum)\n",
    "    print()\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "        saver.save(sess,\"../data/models/{}/model_{}\".format(model_name,one_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    saver.save(sess,\"../data/models/{}_model_{}\".format(model_name,one_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 10, 9, 14)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/update_model/model_2'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"models/{}/model_{}\".format(model_name,one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 meta meta 38213816  5月 30 15:09 models/update_model/model_2.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l 'models/update_model/model_2.data-00000-of-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'update_model'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in ['data-00000-of-00001','meta','index']:\n",
    "    src = \"models/{}/model_{}.{}\".format(model_name,one_epoch,f)\n",
    "    dst = os.path.join(network_dir,\"{}.{}\".format(stamp,f))\n",
    "    shutil.copyfile(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-05-24_21-19-55', '2018-05-23']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([i[:-6] for i in os.listdir('data/prepare_weight/') if '.index' in i])[::-1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "new_name, old_name = sorted([i[:-6] for i in os.listdir('data/prepare_weight/') if '.index' in i])[::-1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-05-30_18-41-03', '2018-05-30_16-00-13')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_name, old_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf1.3_python",
   "language": "python",
   "name": "tf1.3_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
